<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Udacity DeepRL project 1: Navigation :: Gregor the coding cat — A blog about CS stuff</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="Using DQN to solve the Banana environment from ML-Agents In this post we will look at how to implement an agent that uses the DQN algorithm to solve a simple navigation tasks from the ml-agents package. This post is part of the submission for the project-1 of the Deep Reinforcement Learning Nanodegree by Udacity.
Outline These are the following topics we will cover:
 A quick overview of RL and DeepRL."/>
<meta name="keywords" content=""/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="https://wpumacay.github.io/research_blog/posts/deeprl-pt0-intro/" />


<link rel="stylesheet" href="https://wpumacay.github.io/research_blog/assets/style.css">


<link rel="stylesheet" href="https://wpumacay.github.io/research_blog/style.css">


<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://wpumacay.github.io/research_blog/img/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="https://wpumacay.github.io/research_blog/img/favicon.png">


<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Udacity DeepRL project 1: Navigation :: Gregor the coding cat — A blog about CS stuff" />
<meta name="twitter:description" content="Using DQN to solve the Banana environment from ML-Agents In this post we will look at how to implement an agent that uses the DQN algorithm to solve a simple navigation tasks from the ml-agents package. This post is part of the submission for the project-1 of the Deep Reinforcement Learning Nanodegree by Udacity.
Outline These are the following topics we will cover:
 A quick overview of RL and DeepRL." />
<meta name="twitter:site" content="https://wpumacay.github.io/research_blog/" />
<meta name="twitter:creator" content="" />
<meta name="twitter:image" content="">


<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Udacity DeepRL project 1: Navigation :: Gregor the coding cat — A blog about CS stuff">
<meta property="og:description" content="Using DQN to solve the Banana environment from ML-Agents In this post we will look at how to implement an agent that uses the DQN algorithm to solve a simple navigation tasks from the ml-agents package. This post is part of the submission for the project-1 of the Deep Reinforcement Learning Nanodegree by Udacity.
Outline These are the following topics we will cover:
 A quick overview of RL and DeepRL." />
<meta property="og:url" content="https://wpumacay.github.io/research_blog/posts/deeprl-pt0-intro/" />
<meta property="og:site_name" content="Udacity DeepRL project 1: Navigation" />
<meta property="og:image" content="">
<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">

<meta property="article:published_time" content="2019-05-06 15:35:13 -0500 -05" />







</head>
<body class="">
<div class="container">
  <header class="header">
  <span class="header__inner">
    <a href="/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" width="44" height="44" viewBox="0 0 44 44">
  <polyline fill="none" stroke="#000" stroke-width="2" points="15 8 29.729 22.382 15 35.367"/>
</svg>
</span>
    <span class="logo__text">cd home</span>
    <span class="logo__cursor"></span>
  
</a>

    <span class="header__right">
      
      <span class="theme-toggle">
        <svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>

      </span>
    </span>
  </span>
</header>


  <div class="content">
    
  <div class="post">
    <h2 class="post-title"><a href="https://wpumacay.github.io/research_blog/posts/deeprl-pt0-intro/">Udacity DeepRL project 1: Navigation</a></h2>
    <div class="post-meta">
      
        <span class="post-date">
            2019-05-06
        </span>
      
      
      
    </div>

    

    

    <div class="post-content">
      <h1 id="using-dqn-to-solve-the-banana-environment-from-mlagents">Using DQN to solve the Banana environment from ML-Agents</h1>

<p>In this post we will look at how to implement an agent that uses the DQN algorithm
to solve a simple navigation tasks from the <a href="https://github.com/Unity-Technologies/ml-agents">ml-agents</a>
package. This post is part of the submission for the project-1 of the
<a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893"><strong>Deep Reinforcement Learning Nanodegree</strong></a>
by <em>Udacity</em>.</p>

<h2 id="outline">Outline</h2>

<p>These are the following topics we will cover:</p>

<ul>
<li>A quick overview of RL and DeepRL.</li>
<li>The DQN algorithm in more detail.</li>
<li>An overview of the Banana environment.</li>
<li>Implementation details.</li>
<li>Improvements: Double DQN.</li>
<li>Improvements: Prioritized experience replay.</li>
<li>Results.</li>
<li>Future improvements.</li>
</ul>

<h2 id="a-quick-overview-of-rl-and-deeprl">A quick overview of RL and DeepRL</h2>

<h3 id="reinforcement-learning-the-problem">Reinforcement learning: the problem</h3>

<p>Reinforcement Learning (RL) is a learning approach in which an <strong>Agent</strong> learns by
<em>trial and error</em> while interacting with an <strong>Environment</strong>. The core setup is shown
in Figure 1, where we have an agent in a certain <strong>state</strong> \( S_{t} \)
interacting with an environment by applying some <strong>action</strong> \( A_{t} \).
Because of this interaction, the agent receives a reward \( R_{t+1} \) from
the environment and it also ends up in a new state \( S_{t+1} \).</p>


  <figure class="center" >
    <img src="/imgs/img_rl_loop.png"  alt="img-rl-loop"   style="border-radius: 8px;"  />
    
      <figcaption class="center"  style="color: black;" >Figure 1. RL interaction loop</figcaption>
    
  </figure>



<p>This can be further formalize using the framework of Markov Decision Proceses (MDPs).
Using this framework we can define our RL problem as follows:</p>

<blockquote>
<p>A Markov Decision Process (MDP) is defined as a tuple of the following components:</p>

<ul>
<li><strong>A state space</strong> \( \mathbb{S} \) of configurations \( s_{t} \) for an agent.</li>
<li><strong>An action space</strong> \( \mathbb{A} \) of actions \( a_{t} \) that the agent can take in the environment.</li>
<li><strong>A transition model</strong> \( p(s',r | s,a) \) that defines the distribution of states
that an agent can land on and rewards it can obtain \( s',r \) by taking an
action \( a \) in a state \( s \).</li>
</ul>
</blockquote>

<p>The objective of the agent is to maximize the <strong>total sum of rewards</strong> that it can
get from its interactions, and because the environment can potentially be stochastic
(recall that the transition model defines a probability distribution) the objective
is usually formulated as an <strong>expectation</strong> ( \( \mathbb{E}  \) ) over the random
variable defined by the total sum of rewards. Mathematically, this is described in
the following equation.</p>

<p><span  class="math">\[
\max \mathbb{E} \left \{ r_{t+1} + r_{t+2} + r_{t+3} + \dots \right \}
\]</span></p>

<h3 id="reinforcement-learning-the-solution">Reinforcement learning: the solution</h3>

<p>A solution to the RL problem consists of a <strong>Policy</strong> \( \pi \), which is a mapping
from the current state we are ( \( s_{t} \) ) to an appropriate action ( \( a_{t} \) )
from the action space. Such mapping is basically a function, and we can defined it as follows:</p>

<blockquote>
<p>A <strong>deterministic</strong> policy is a mapping \( \pi : \mathbb{S} \rightarrow \mathbb{A} \)
that returns an action to take \( a_{t} \) from a given state \( s_{t} \).</p>

<p><span  class="math">\[
a = \pi(s)
\]</span></p>
</blockquote>

<p>We could also define an stochastic policy</p>

    </div>
    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>
        <div class="pagination__buttons">
          
          
            <span class="button next">
              <a href="https://wpumacay.github.io/research_blog/posts/deeprlnd-project1-navigation/">
                <span class="button__text">Udacity DeepRL project 1: Navigation</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    

    

    </div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <a href="/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" width="44" height="44" viewBox="0 0 44 44">
  <polyline fill="none" stroke="#000" stroke-width="2" points="15 8 29.729 22.382 15 35.367"/>
</svg>
</span>
    <span class="logo__text">cd home</span>
    <span class="logo__cursor"></span>
  
</a>

      <div class="copyright">
        <span>© 2019 Powered by <a href="http://gohugo.io">Hugo</a></span>
        <span>Theme created by <a href="https://twitter.com/panr">panr</a></span>
      </div>
    
  </div>
</footer>

<script src="https://wpumacay.github.io/research_blog/assets/main.js"></script>
<script src="https://wpumacay.github.io/research_blog/assets/prism.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css" integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js" integrity="sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
  
</div>

</body>
</html>
