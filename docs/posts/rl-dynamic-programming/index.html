<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Rl Dynamic Programming :: Gregor the coding cat — A blog about CS stuff</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="Planning by Dynamic Programming Introduction The first set of methods we will study are exact methods, which can be used when we have a perfect model of the dynamics of the environment, namely the joint distribution \( p(s&#39;,r|s,a) \) defined in our MDP. By having a model we will take advantage of the recursive form of our problem, given by the Bellman Equations studied earlier, and solve for the State-Value function ( \( V(s) \) ) and Action-Value function ( \( Q(s,a) \) ) using the following iterative methods:"/>
<meta name="keywords" content=""/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="https://wpumacay.github.io/research_blog/posts/rl-dynamic-programming/" />


<link rel="stylesheet" href="https://wpumacay.github.io/research_blog/assets/style.css">


<link rel="stylesheet" href="https://wpumacay.github.io/research_blog/style.css">


<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://wpumacay.github.io/research_blog/img/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="https://wpumacay.github.io/research_blog/img/favicon.png">


<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Rl Dynamic Programming :: Gregor the coding cat — A blog about CS stuff" />
<meta name="twitter:description" content="Planning by Dynamic Programming Introduction The first set of methods we will study are exact methods, which can be used when we have a perfect model of the dynamics of the environment, namely the joint distribution \( p(s&#39;,r|s,a) \) defined in our MDP. By having a model we will take advantage of the recursive form of our problem, given by the Bellman Equations studied earlier, and solve for the State-Value function ( \( V(s) \) ) and Action-Value function ( \( Q(s,a) \) ) using the following iterative methods:" />
<meta name="twitter:site" content="https://wpumacay.github.io/research_blog/" />
<meta name="twitter:creator" content="" />
<meta name="twitter:image" content="">


<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Rl Dynamic Programming :: Gregor the coding cat — A blog about CS stuff">
<meta property="og:description" content="Planning by Dynamic Programming Introduction The first set of methods we will study are exact methods, which can be used when we have a perfect model of the dynamics of the environment, namely the joint distribution \( p(s&#39;,r|s,a) \) defined in our MDP. By having a model we will take advantage of the recursive form of our problem, given by the Bellman Equations studied earlier, and solve for the State-Value function ( \( V(s) \) ) and Action-Value function ( \( Q(s,a) \) ) using the following iterative methods:" />
<meta property="og:url" content="https://wpumacay.github.io/research_blog/posts/rl-dynamic-programming/" />
<meta property="og:site_name" content="Rl Dynamic Programming" />
<meta property="og:image" content="">
<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">

<meta property="article:published_time" content="2019-04-01 23:31:27 -0500 -05" />







</head>
<body class="">
<div class="container">
  <header class="header">
  <span class="header__inner">
    <a href="/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" width="44" height="44" viewBox="0 0 44 44">
  <polyline fill="none" stroke="#000" stroke-width="2" points="15 8 29.729 22.382 15 35.367"/>
</svg>
</span>
    <span class="logo__text">cd home</span>
    <span class="logo__cursor"></span>
  
</a>

    <span class="header__right">
      
      <span class="theme-toggle">
        <svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>

      </span>
    </span>
  </span>
</header>


  <div class="content">
    
  <div class="post">
    <h2 class="post-title"><a href="https://wpumacay.github.io/research_blog/posts/rl-dynamic-programming/">Rl Dynamic Programming</a></h2>
    <div class="post-meta">
      
        <span class="post-date">
            2019-04-01
        </span>
      
      
      
    </div>

    

    

    <div class="post-content">
      <h1 id="planning-by-dynamic-programming">Planning by Dynamic Programming</h1>

<h2 id="introduction">Introduction</h2>

<p>The first set of methods we will study are exact methods, which can be used
when we have a perfect model of the dynamics of the environment, namely the
joint distribution \( p(s',r|s,a) \) defined in our MDP. By having a model
we will take advantage of the recursive form of our problem, given by the
<em>Bellman Equations</em> studied earlier, and solve for the <em>State-Value function</em> ( \( V(s) \) )
and <em>Action-Value function</em> ( \( Q(s,a) \) ) using the following iterative methods:</p>

<ul>
<li><strong>Policy Evaluation</strong></li>
<li><strong>Policy Iterarion</strong></li>
<li><strong>Value Iteration</strong></li>
</ul>

<h2 id="exploiting-our-knowledge-of-the-environment">Exploiting our knowledge of the environment</h2>

<p>Let's try to exploit our knowledge of the transition dynamics \( p(s',r|s,a) \). Recall
the <strong>Bellman Equations</strong> from the previous post:</p>

<ul>
<li><strong>Bellman Expectation Equation</strong></li>
</ul>

<p><span  class="math">\[
V^{\pi}(s) = \mathbb{E}_{\pi} \left \{ R_{t} + \gamma V^{\pi}(s') | s_{t}=s  \right \}
\]</span></p>

<ul>
<li><strong>Bellman Optimality Equation</strong></li>
</ul>

<p><span  class="math">\[
V^{*}(s) = \max_{\pi} \mathbb{E}_{\pi} \left \{ R_{t} + \gamma V^{*}(s') | s_{t}=s \right \}
\]</span></p>

<p>The nature of the expectations in the equations above is caused by the fact that
the internal quantities that appear recursively ( \( R_{t} + \gamma V(s') \) )
are random variables, whose distribution is induced by the policy \( pi \) we
follow (which can be deterministic or sthocastic) and the dynamics of the environment
\( p(s',r|s,a) \).</p>

<p>In the case we try to solve, we are given the dynamics of the environment, so we
know how the future will unroll once we take a transition \( (s,a) \). This means
that we know the probability for which the terms \( R_{t} + \gamma V(s') \) will
appear, which allows us to compute the expectation.</p>

    </div>
    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>
        <div class="pagination__buttons">
          
            <span class="button previous">
              <a href="https://wpumacay.github.io/research_blog/posts/deeprlnd-project1-navigation/">
                <span class="button__icon">←</span>
                <span class="button__text">Udacity DeepRL project 1: Navigation</span>
              </a>
            </span>
          
          
            <span class="button next">
              <a href="https://wpumacay.github.io/research_blog/posts/hello-world/">
                <span class="button__text">Hello World</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    

    

    </div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <a href="/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" width="44" height="44" viewBox="0 0 44 44">
  <polyline fill="none" stroke="#000" stroke-width="2" points="15 8 29.729 22.382 15 35.367"/>
</svg>
</span>
    <span class="logo__text">cd home</span>
    <span class="logo__cursor"></span>
  
</a>

      <div class="copyright">
        <span>© 2019 Powered by <a href="http://gohugo.io">Hugo</a></span>
        <span>Theme created by <a href="https://twitter.com/panr">panr</a></span>
      </div>
    
  </div>
</footer>

<script src="https://wpumacay.github.io/research_blog/assets/main.js"></script>
<script src="https://wpumacay.github.io/research_blog/assets/prism.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css" integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js" integrity="sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
  
</div>

</body>
</html>
